{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Generation.ipynb  \u001b[1m\u001b[34mdumps\u001b[m\u001b[m/                 ptb.vocab.json\r\n",
      "README.md              \u001b[1m\u001b[34mfigs\u001b[m\u001b[m/                  requirements.txt\r\n",
      "\u001b[1m\u001b[34m__pycache__\u001b[m\u001b[m/           inference.py           \u001b[1m\u001b[34msimple-examples\u001b[m\u001b[m/\r\n",
      "\u001b[1m\u001b[34mbin\u001b[m\u001b[m/                   model.py               simple-examples.tgz\r\n",
      "\u001b[1m\u001b[34mdata\u001b[m\u001b[m/                  ptb.py                 train.py\r\n",
      "\u001b[1m\u001b[34mdata-copy\u001b[m\u001b[m/             ptb.train.txt          utils.py\r\n",
      "\u001b[1m\u001b[31mdowloaddata.sh\u001b[m\u001b[m*        ptb.valid.txt\r\n"
     ]
    }
   ],
   "source": [
    "ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "clean_poems.csv      ptb.train.json       ptb.vocab.json\r\n",
      "final_poems.csv      ptb.train.txt        ultimate_poems.csv\r\n",
      "glove.6B.300d.txt    ptb.valid.json\r\n",
      "glove.840B.300d.txt  ptb.valid.txt\r\n"
     ]
    }
   ],
   "source": [
    "ls data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9877\n",
      "9877\n",
      "w2i\n",
      "i2w\n"
     ]
    }
   ],
   "source": [
    "with open('data-copy/ptb.vocab.json') as data_file:    \n",
    "    data = json.load(data_file)\n",
    "    \n",
    "print(len(data['w2i']))\n",
    "print(len(data['i2w']))\n",
    "#print(data)\n",
    "for k, v in data.items():\n",
    "    print(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2195884\n",
      ".\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "# prepare glove vectors\n",
    "words = set()\n",
    "word_to_vec = {}\n",
    "word_to_id = {}\n",
    "id_to_word = {}\n",
    "glove_dim = 300\n",
    "glove_filename = \"data/glove.840B.300d.txt\" # \"./glove/glove.6B.300d.txt\"\n",
    "with open(glove_filename) as file:\n",
    "    count = 0\n",
    "    for line in file:\n",
    "        tokens = line.split()\n",
    "        word, vals = tokens[0], tokens[-glove_dim:]\n",
    "        if word not in words:\n",
    "            words.add(word)\n",
    "            #word_to_vec[word] = torch.Tensor([float(val) for val in vals])\n",
    "            word_to_id[word] = count\n",
    "            id_to_word[count] = word\n",
    "            count += 1\n",
    "print(len(words))\n",
    "print(id_to_word[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_embedding(words, id_to_word, word_to_vec, dim):\n",
    "    weights = torch.stack([word_to_vec[id_to_word[i]] for i in range(len(words))])\n",
    "    embed = nn.Embedding.from_pretrained(weights)\n",
    "    return embed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(vocab, poems, limit):\n",
    "    nlp = spacy.load('en')\n",
    "    line_data = []\n",
    "    scraps = {}\n",
    "    count = 0\n",
    "    for poem in poems:\n",
    "        try:\n",
    "            for line in poem.lower().split('\\n'):\n",
    "                if len(line) == 0:\n",
    "                    continue\n",
    "                tokens = nlp(line)\n",
    "                include = True\n",
    "                bad_words = []\n",
    "                for token in tokens:\n",
    "                    if token.text not in vocab:\n",
    "                        bad_words.append(token.text)\n",
    "                        include = False\n",
    "                if include: line_data.append(line)\n",
    "                else: scraps[line] = bad_words\n",
    "        except:\n",
    "            continue\n",
    "        if count == limit:\n",
    "            break\n",
    "        count += 1\n",
    "    return line_data, scraps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15 1\n",
      "{'and discrete-a mirror come unsilvered,': ['unsilvered']}\n"
     ]
    }
   ],
   "source": [
    "def prepare_training_data(limit=-1):\n",
    "    filename = \"./data/ultimate_poems.csv\"\n",
    "    data = pd.read_csv(filename)\n",
    "    #poems = data.iloc[:,3]\n",
    "    poems = load_data(words, data, limit)\n",
    "    return poems\n",
    "    \n",
    "def sentence_to_vector(s, word_to_vec):\n",
    "    ids = [word_to_vec[w] for w in s]\n",
    "    return torch.tensor(ids, dtype=torch.long)\n",
    "\n",
    "def sentence_one_hot(s, word_to_vec):\n",
    "    one_hot = torch.zeros([len(s), len(word_to_vec)], dtype=torch.long)\n",
    "    for i, w in enumerate(s):\n",
    "        one_hot[i, word_to_vec[w]] = 1\n",
    "    return one_hot\n",
    "\n",
    "train_data, scraps = prepare_training_data(limit=-1)\n",
    "print(len(train_data), len(scraps))\n",
    "print(scraps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en')\n",
    "filename = \"data/ultimate_poems.csv\"\n",
    "poems = pd.read_csv(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8722\n",
      "4155\n"
     ]
    }
   ],
   "source": [
    "good_poems = []\n",
    "bad_poems  = []\n",
    "\n",
    "def valid_line(line, words):\n",
    "    tokens = nlp(line)\n",
    "    count = 0\n",
    "    for token in tokens:\n",
    "        w = token.text\n",
    "        if w.isalnum():\n",
    "            if w not in words:\n",
    "                return False\n",
    "            count += 1\n",
    "    if count < 2:\n",
    "        return False\n",
    "    return True\n",
    "            \n",
    "for poem in poems.iloc[:,1]:\n",
    "    if valid_line(poem, words): good_poems.append(poem)\n",
    "    else: bad_poems.append(poem)\n",
    "print(len(good_poems))\n",
    "print(len(bad_poems))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                               poems\n",
      "0  we'd  like  to  talk  with  you  about  fear t...\n",
      "1  the wise men will unlearn your name.\\nabove yo...\n",
      "2  winter\\nmore time is spent at the window.\\n\\ns...\n",
      "3  may i never be afraid\\nespecially of myself\\nb...\n",
      "4  the shift of sleepwalks and suicides.\\nthe occ...\n"
     ]
    }
   ],
   "source": [
    "lower_poems = [poem.lower() for poem in good_poems]\n",
    "df = pd.DataFrame({'poems':lower_poems})\n",
    "print(df.head())\n",
    "df.to_csv(path_or_buf=\"./data/final_poems.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "clean_poems.csv      glove.840B.300d.txt  ptb.vocab.json\r\n",
      "final_poems.csv      ptb.train.txt        ultimate_poems.csv\r\n",
      "glove.6B.300d.txt    ptb.valid.txt\r\n"
     ]
    }
   ],
   "source": [
    "ls data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/johnhallman/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "poem_tokens = [word_tokenize(poem) for poem in lower_poems]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ptb.test.txt    ptb.train.txt   ptb.valid.txt\r\n",
      "ptb.train.json  ptb.valid.json  ptb.vocab.json\r\n"
     ]
    }
   ],
   "source": [
    "ls data-copy/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "54838\n"
     ]
    }
   ],
   "source": [
    "vocab = set()\n",
    "for tokens in poem_tokens:\n",
    "    for token in tokens:\n",
    "        vocab.add(token)\n",
    "print(len(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "54838\n",
      "26428\n"
     ]
    }
   ],
   "source": [
    "vocab_count = {}\n",
    "for tokens in poem_tokens:\n",
    "    for token in tokens:\n",
    "        if token not in vocab_count:\n",
    "            vocab_count[token] = 1\n",
    "        else:\n",
    "            vocab_count[token] += 1\n",
    "print(len(vocab_count))\n",
    "randcount = set()\n",
    "for k, v in vocab_count.items():\n",
    "    if v == 1:\n",
    "        randcount.add(k)\n",
    "print(len(randcount))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_poem_tokens = []\n",
    "for tokens in poem_tokens:\n",
    "    temp_poem_tokens.append(['<unk>' if token in randcount else token for token in tokens])\n",
    "\n",
    "def ascii_word(w):\n",
    "    for c in w:\n",
    "        if ord(c) >= 128:\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "new_poem_tokens = []\n",
    "for tokens in temp_poem_tokens:\n",
    "    temp = []\n",
    "    for token in tokens:\n",
    "        if ascii_word(token): temp.append(token)\n",
    "        else: temp.append('<unk>')\n",
    "    new_poem_tokens.append(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['the', 'shift', 'of', '<unk>', 'and', 'suicides', '.', 'the', 'occasion', 'of', 'owls', 'and', 'a', '<unk>', 'fog', '.', 'even', 'god', 'has', 'nodded', 'off', 'and', 'wo', \"n't\", 'be', 'taking', 'prayers', 'til', 'ten', '.', 'ad', 'interim', ',', 'you', 'put', 'them', 'on', '.', 'as', 'if', 'your', 'wants', 'could', 'keep', 'you', 'warm', '.', 'as', 'if', '.', 'you', 'say', 'your', '<unk>', '.', 'you', 'thumb', 'your', 'beads', '.', 'you', '<unk>', 'the', 'glass', '.', 'night', 'creeps', 'to', 'its', 'precipice', 'and', 'the', 'broken', 'rim', 'of', 'reason', 'breaks', 'again', '.', 'an', 'obsidian', 'sky', 'betrays', 'you', '.', 'every', '<unk>', 'shadow', '<unk>', 'you', '.', 'soon', 'enough', ',', 'the', 'crow', 'will', 'caw', '.', 'the', 'cock', 'will', 'crow', '.', 'the', 'door', 'will', 'close', '.', '(', 'he', 'is', \"n't\", 'coming', 'back', ',', 'you', 'know', '.', ')', 'and', 'so', 'wee', ',', 'wet', 'hours', 'of', 'grief', 'relent', '.', 'in', 'thirty', 'years', 'you', 'might', 'forget', 'precisely', 'how', 'tonight', \"'s\", 'pain', 'felt', '.', 'and', 'in', 'whose', 'black', 'house', 'you', 'dwelt', '.']\n"
     ]
    }
   ],
   "source": [
    "print(new_poem_tokens[4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the wise men will unlearn your name . above your head no star will flame . one weary sound will be the same- the hoarse roar of the gale . the shadows fall from your tired eyes as your lone bedside candle dies , for here the calendar breeds nights till stores of candles fail . what prompts this melancholy key ? a long familiar melody . it sounds again . so let it be . let it sound from this night . let it sound in my hour of death- as gratefulness of eyes and lips for that which sometimes makes us lift our gaze to the far sky . you glare in silence at the wall . your stocking gapes : no gifts at all . it 's clear that you are now too old to trust in good saint nick ; that it 's too late for miracles . -but suddenly , lifting your eyes to heaven 's light , you realize : your life is a sheer gift . \n"
     ]
    }
   ],
   "source": [
    "def list_to_line(l):\n",
    "    output = \"\"\n",
    "    for w in l:\n",
    "        output += w + \" \"\n",
    "    return output\n",
    "\n",
    "print(list_to_line(new_poem_tokens[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8722\n",
      "722\n"
     ]
    }
   ],
   "source": [
    "print(len(new_poem_tokens))\n",
    "train = new_poem_tokens[:8000]\n",
    "valid = new_poem_tokens[8000:]\n",
    "print(len(valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"ptb.train.txt\", \"w\") as file:\n",
    "    for s in train:\n",
    "        file.write(list_to_line(s))\n",
    "        file.write('\\n')\n",
    "with open(\"ptb.valid.txt\", \"w\") as file:\n",
    "    for s in valid:\n",
    "        file.write(list_to_line(s))\n",
    "        file.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 0\n",
    "w2i = {}\n",
    "i2w = {}\n",
    "previous = set()\n",
    "special = [\"<pad>\", \"<unk>\", \"<sos>\", \"<eos>\"]\n",
    "for token in special:\n",
    "    w2i[token] = count\n",
    "    i2w[count] = token\n",
    "    count += 1\n",
    "for tokens in poem_tokens:\n",
    "    for token in tokens:\n",
    "        if token not in previous:\n",
    "            w2i[token] = count\n",
    "            i2w[count] = token\n",
    "            previous.add(token)\n",
    "            count += 1\n",
    "final_vocab = {'w2i':w2i, 'i2w':i2w}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('ptb.vocab.json', 'w') as file:\n",
    "    json.dump(final_vocab, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "clean_poems.csv      glove.840B.300d.txt  ptb.vocab.json\r\n",
      "final_poems.csv      ptb.train.txt        ultimate_poems.csv\r\n",
      "glove.6B.300d.txt    ptb.valid.txt\r\n"
     ]
    }
   ],
   "source": [
    "ls data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['This', 'is', 'a', 'test']\n"
     ]
    }
   ],
   "source": [
    "test = \"This is\\na test\"\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "print(word_tokenize(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(\"./data/final_poems.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "final_lines = []\n",
    "for poem in df.iloc[:,1]:\n",
    "    lines = poem.split('\\n')\n",
    "    for line in lines:\n",
    "        tokens = word_tokenize(line)\n",
    "        if len(tokens) >= 2:\n",
    "            final_lines.append(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "54107\n"
     ]
    }
   ],
   "source": [
    "vocab = set()\n",
    "for line in final_lines:\n",
    "    for token in line:\n",
    "        vocab.add(token)\n",
    "print(len(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "201824\n"
     ]
    }
   ],
   "source": [
    "print(len(final_lines))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
